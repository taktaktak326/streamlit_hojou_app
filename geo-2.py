# streamlit_app.py
# å¿…è¦: pip install streamlit geopandas shapely folium streamlit-folium xlsxwriter rtree

import re, math
from io import BytesIO

import geopandas as gpd
import pandas as pd
import streamlit as st
import folium
from shapely.geometry.base import BaseGeometry
from streamlit_folium import folium_static

# ========== Page / Session ==========
st.set_page_config(page_title="è¾²åœ°ãƒ”ãƒ³Ã—ç­†ãƒãƒªã‚´ãƒ³ çµåˆ", layout="wide")
for k, v in {
    "sheet_name": None, "header_row": None, "addr_col": None, "excel_hash": None,
    "joined": None  # â† ç©ºé–“çµåˆã®çµæœã‚’ä¿æŒ
}.items():
    st.session_state.setdefault(k, v)

# ========== Utils ==========
HYPHENS = r"[â€-â€’â€“â€”â€•ãƒ¼ï¼-]"
HEADER_HINTS = ["ä½æ‰€åœ°ç•ª","ä½æ‰€","åœ°ç•ª","ç­†","åœ°ç›®","é¢ç©","åœƒå ´","è¾²åœ°","å­—","ç•ªåœ°"]

def to_half(s): return s.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™","0123456789")) if isinstance(s,str) else s

def norm_addr_key(s: str) -> str:
    if not isinstance(s, str) or pd.isna(s): return ""
    s = to_half(s.strip()).lower().replace("ã€€"," ")
    s = re.sub(HYPHENS,"-", s); s = re.sub(r"\s+","", s)
    s = s.replace("ä¸ç›®","-").replace("ç•ªåœ°","-").replace("ç•ª","-").replace("å·","")
    return re.sub(r"-\d{1,4}$","", s)

def addr_key_loose(k: str) -> str:
    s = re.sub(r"(æ±äº¬éƒ½|åŒ—æµ·é“|äº¬éƒ½åºœ|å¤§é˜ªåºœ|..çœŒ|..éƒ½|..é“|..åºœ)","", k)
    return re.sub(r".{1,6}(å¸‚|åŒº|ç”º|æ‘)","", s)

def score_header_row(vals) -> int:
    score = 0
    for v in vals:
        s = str(v)
        score += 2*sum(h in s for h in HEADER_HINTS)
        if 2 <= len(s) <= 12 and re.fullmatch(r"[^\d\s]{2,}", s or ""): score += 1
    return score

def suggest_header_rows(pre: pd.DataFrame, topk=6):
    n = min(40, len(pre))
    cand = sorted([(i, score_header_row(pre.iloc[i].values)) for i in range(n)],
                  key=lambda x: x[1], reverse=True)
    return [i for i,sc in cand[:topk] if sc>0]

@st.cache_data(show_spinner=False)
def read_geojson(files):
    gdfs = []
    for f in files:
        gdf = gpd.read_file(f)
        gdf = (gdf.set_crs(epsg=4326, allow_override=True) if gdf.crs is None else gdf.to_crs(4326))
        gdfs.append(gdf)
    return gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs="EPSG:4326")

@st.cache_data(show_spinner=False)
def sjoin_pori_pin(_g_pori: gpd.GeoDataFrame, _g_pin: gpd.GeoDataFrame):
    """GeoJSONã®ç©ºé–“çµåˆã€‚GeoDataFrameã¯unhashableãªã®ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥éµã‹ã‚‰é™¤å¤–"""
    try:
        j = gpd.sjoin(_g_pori, _g_pin, predicate="covers", how="left")
    except Exception:
        j = gpd.sjoin(_g_pori, _g_pin, predicate="intersects", how="left")
    return j.drop_duplicates()


def gdf_signature(gdf: gpd.GeoDataFrame, addr_col: str) -> tuple:
    bounds = tuple(map(float, gdf.total_bounds)); n = int(len(gdf))
    h = int(pd.util.hash_pandas_object(gdf[addr_col].astype(str), index=False).sum()) if addr_col in gdf.columns else 0
    return (n, bounds, h)

# GeoDataFrameã¯unhashable â†’ ç¬¬1å¼•æ•°ã‚’ _gdf ã«ã—ã¦éµã‹ã‚‰é™¤å¤–ã€ä»£ã‚ã‚Šã« gdf_sig ã‚’ä½¿ã†
@st.cache_data(show_spinner=False)
def build_addr_dict(_gdf: gpd.GeoDataFrame, col: str, gdf_sig: tuple):
    t = _gdf[[col,"geometry"]].dropna(subset=[col,"geometry"]).copy()  # ä½æ‰€NaN/geomNaNé™¤å¤–
    t["k1"] = t[col].astype(str).map(norm_addr_key)
    t["k2"] = t["k1"].map(addr_key_loose)
    return (t.groupby("k1")["geometry"].first().to_dict(),
            t.groupby("k2")["geometry"].first().to_dict())

def apply_match(df: pd.DataFrame, col: str, d1: dict, d2: dict):
    out = df.copy()
    out["__k"]  = out[col].astype(str).map(norm_addr_key)
    out["geom"] = out["__k"].map(d1)
    miss        = out["geom"].isna()
    out.loc[miss,"geom"] = out.loc[miss,"__k"].map(addr_key_loose).map(d2)
    out["geom"] = out["geom"].apply(lambda g: g if isinstance(g, BaseGeometry) else "ä¸€è‡´ãªã—")
    return out.drop(columns="__k")

def to_excel_bytes(df: pd.DataFrame) -> BytesIO:
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="xlsxwriter") as w:
        df.to_excel(w, sheet_name="MatchedData", index=False)
    bio.seek(0); return bio

def to_wkt_safe(g): return g.wkt if isinstance(g, BaseGeometry) else ""

def paginate(df: pd.DataFrame, page: int, page_size: int) -> pd.DataFrame:
    start = (page-1) * page_size
    return df.iloc[start:start+page_size]

# ========== UI ==========
st.title("è¾²åœ°ãƒ”ãƒ³ã¨ç­†ãƒãƒªã‚´ãƒ³ã®çµåˆ & åœƒå ´ç™»éŒ²ä»£è¡Œã‚·ãƒ¼ãƒˆã®çµ±åˆ")

# ---- A. GeoJSONã‚’ã‚¢ãƒƒãƒ— â†’ ç©ºé–“çµåˆã‚’è‡ªå‹•å®Ÿè¡Œ â†’ ä½æ‰€ã‚«ãƒ©ãƒ ã‚’é¸æŠ ----
left, right = st.columns(2)
with left:
    st.subheader("1ï¸âƒ£ GeoJSONã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    f_pori = st.file_uploader("ç­†ãƒãƒªã‚´ãƒ³ï¼ˆè¤‡æ•°å¯ï¼‰", type=["geojson"], accept_multiple_files=True, key="poris")
    f_pins = st.file_uploader("è¾²åœ°ãƒ”ãƒ³ï¼ˆè¤‡æ•°å¯ï¼‰", type=["geojson"], accept_multiple_files=True, key="pins")

with right:
    st.subheader("2ï¸âƒ£ åœƒå ´ç™»éŒ²ä»£è¡Œã‚·ãƒ¼ãƒˆï¼ˆExcelï¼‰")
    f_xlsx = st.file_uploader("Excelã‚’é¸æŠ", type=["xlsx","xls"], key="xlsx")

# GeoJSONãŒãã‚ã£ãŸã‚‰ç©ºé–“çµåˆã‚’å®Ÿæ–½ï¼ˆâ€œå‡¦ç†é–‹å§‹â€å‰ã«æº–å‚™ã™ã‚‹ï¼‰
if f_pori and f_pins:
    try:
        g_pori = read_geojson(f_pori); g_pin = read_geojson(f_pins)
        st.session_state.joined = sjoin_pori_pin(g_pori, g_pin)
        st.success("âœ… ç©ºé–“çµåˆã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚ä¸‹ã§ã€ä½æ‰€ã«ä½¿ã†ã‚«ãƒ©ãƒ ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        st.caption("ç©ºé–“çµåˆï¼ˆå…ˆé ­5ä»¶ï¼‰")
        st.dataframe(st.session_state.joined.head(), use_container_width=True)
    except Exception as e:
        st.error(f"ç©ºé–“çµåˆã§ã‚¨ãƒ©ãƒ¼: {e}")
        st.session_state.joined = None

# ä½æ‰€ã‚«ãƒ©ãƒ é¸æŠï¼ˆç©ºé–“çµåˆãŒæ¸ˆã‚“ã§ã„ã‚‹å‰æã§ã€å‡¦ç†å‰ã«é¸ã¶ï¼‰
if st.session_state.joined is not None and not st.session_state.joined.empty:
    jcols = list(st.session_state.joined.columns)
    candidates = ["ä½æ‰€","ä½æ‰€åœ°ç•ª","Address","address","location","name"]
    auto = next((c for c in candidates if c in jcols), jcols[0])
    if st.session_state.addr_col not in jcols: st.session_state.addr_col = auto
    st.subheader("3ï¸âƒ£ ä½æ‰€ã«ä½¿ã†ã‚«ãƒ©ãƒ ã‚’é¸æŠ")
    st.session_state.addr_col = st.selectbox(
        "ä½æ‰€ã‚«ãƒ©ãƒ ", jcols, index=jcols.index(st.session_state.addr_col), help="ä½æ‰€ãƒ»ä½æ‰€åœ°ç•ªãƒ»location ç­‰"
    )

st.divider()

# ---- B. Excelãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®é¸æŠï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‹å€™è£œï¼‰ ----
if f_xlsx:
    h = hash(f_xlsx.getvalue())
    if st.session_state.excel_hash != h:
        st.session_state.update({"sheet_name": None, "header_row": None, "excel_hash": h})
    try:
        xls = pd.ExcelFile(f_xlsx); sheets = xls.sheet_names
    except Exception as e:
        st.error(f"Excelã®ã‚·ãƒ¼ãƒˆå–å¾—ã«å¤±æ•—: {e}")
        sheets = []
    if sheets:
        idx = sheets.index(st.session_state.sheet_name) if st.session_state.sheet_name in sheets else 0
        st.session_state.sheet_name = st.selectbox("ã‚·ãƒ¼ãƒˆå", sheets, index=idx)
        pre = pd.read_excel(f_xlsx, sheet_name=st.session_state.sheet_name, header=None, nrows=40)
        st.caption("Excelãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€åˆã®40è¡Œï¼‰")
        st.dataframe(pre, use_container_width=True, height=280)

        st.markdown("**ğŸ§  ãƒ˜ãƒƒãƒ€ãƒ¼å€™è£œï¼ˆä½æ‰€/åœ°ç•ª/ç­†ãªã©ã‚’å„ªå…ˆï¼‰**")
        cand = suggest_header_rows(pre); default_header = cand[0] if cand else 0
        c1, c2 = st.columns([2,3])
        with c1:
            hdr = st.number_input("ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆ0å§‹ã¾ã‚Šï¼‰", 0, len(pre)-1,
                                  value=st.session_state.header_row if st.session_state.header_row is not None else default_header, step=1)
            if cand:
                pick = st.radio("å€™è£œ", options=cand, index=0, format_func=lambda i: f"è¡Œ {i}ï¼ˆå€™è£œï¼‰")
                hdr = pick
            st.session_state.header_row = int(hdr)
        with c2:
            try:
                tmp = pd.read_excel(f_xlsx, sheet_name=st.session_state.sheet_name,
                                    header=st.session_state.header_row, nrows=10)
                st.caption("ãƒ˜ãƒƒãƒ€ãƒ¼é©ç”¨å¾Œãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆä¸Šä½10è¡Œï¼‰")
                st.dataframe(tmp, use_container_width=True, height=240)
                likely = [c for c in tmp.columns if any(h in str(c) for h in ["ä½æ‰€åœ°ç•ª","ä½æ‰€","åœ°ç•ª"])]
                if likely: st.info("ä½æ‰€ã£ã½ã„åˆ—å€™è£œ: " + ", ".join(map(str, likely)))
            except Exception as e:
                st.error(f"ãƒ˜ãƒƒãƒ€ãƒ¼é©ç”¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ã‚¨ãƒ©ãƒ¼: {e}")

# ---- C. ã€Œãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã€ãƒœã‚¿ãƒ³ï¼ˆç©ºé–“çµåˆï¼†ä½æ‰€ã‚«ãƒ©ãƒ é¸æŠã®å¾Œï¼‰ ----
with st.form("match_form"):
    submitted = st.form_submit_button("ğŸš€ ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹", use_container_width=True)

if submitted:
    prog = st.progress(0); msg = st.empty()
    TOTAL_STEPS = 5
    def tick(i): prog.progress(int(i*100/TOTAL_STEPS))

    # å‰æãƒã‚§ãƒƒã‚¯ï¼šç©ºé–“çµåˆã¨ä½æ‰€ã‚«ãƒ©ãƒ 
    if st.session_state.joined is None or st.session_state.addr_col is None:
        st.error("å…ˆã«GeoJSONã‚’ã‚¢ãƒƒãƒ—ã—ã¦ç©ºé–“çµåˆã‚’å®Œäº†ã—ã€ã€ä½æ‰€ã‚«ãƒ©ãƒ ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"); st.stop()
    if not f_xlsx or not st.session_state.sheet_name or st.session_state.header_row is None:
        st.error("Excelã®ã‚·ãƒ¼ãƒˆã¨ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"); st.stop()

    # 1) Excelèª­è¾¼ & ä½æ‰€NaNé™¤å¤–
    msg.text("Excelèª­ã¿è¾¼ã¿â€¦")
    try:
        df = pd.read_excel(f_xlsx, sheet_name=st.session_state.sheet_name, header=st.session_state.header_row)
    except Exception as e:
        st.error(f"Excelã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}"); st.stop()

    excel_addr = "ä½æ‰€åœ°ç•ª" if "ä½æ‰€åœ°ç•ª" in df.columns else next(
        (c for c in df.columns if any(h in str(c) for h in ["ä½æ‰€","åœ°ç•ª"])), None
    )
    if not excel_addr:
        st.error("Excelã«ã€ä½æ‰€åœ°ç•ª/ä½æ‰€/åœ°ç•ªã€ã«ç›¸å½“ã™ã‚‹åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.write("æ¤œå‡ºåˆ—:", list(df.columns)); st.stop()

    before = len(df)
    df = df.dropna(subset=[excel_addr]).copy()
    dropped = before - len(df)
    if dropped > 0: st.info(f"ä½æ‰€ãŒNaNã® {dropped:,} ä»¶ã‚’é™¤å¤–ã—ã¾ã—ãŸã€‚")
    df[excel_addr] = df[excel_addr].astype(str)
    tick(1)

    # 2) ä½æ‰€è¾æ›¸ï¼ˆjoined + é¸æŠã—ãŸä½æ‰€ã‚«ãƒ©ãƒ ï¼‰
    msg.text("ä½æ‰€è¾æ›¸ã‚’æ§‹ç¯‰ä¸­â€¦")
    sig = gdf_signature(st.session_state.joined, st.session_state.addr_col)
    d1, d2 = build_addr_dict(st.session_state.joined, st.session_state.addr_col, sig)
    tick(2)

    # 3) ç…§åˆï¼‹WKT
    msg.text("ä½æ‰€ç…§åˆâ€¦")
    matched = apply_match(df, excel_addr, d1, d2)
    matched["geometry_wkt"] = matched["geom"].apply(lambda g: g.wkt if isinstance(g, BaseGeometry) else "")
    tick(3)

    # 4) é›†è¨ˆï¼‹ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ä¸€è¦§
    msg.text("é›†è¨ˆãƒ»ä¸€è¦§è¡¨ç¤ºâ€¦")
    ok = (matched["geom"]!="ä¸€è‡´ãªã—").sum()
    tot = len(matched); ng = tot-ok; rate = (ok/tot if tot else 0)
    a,b,c = st.columns(3)
    a.metric("ä¸€è‡´ä»¶æ•°", f"{ok:,}"); b.metric("æœªä¸€è‡´ä»¶æ•°", f"{ng:,}"); c.metric("ä¸€è‡´ç‡", f"{rate:.1%}")

    st.subheader("ğŸ” ç…§åˆçµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­20è¡Œï¼‰")
    st.dataframe(matched.head(20), use_container_width=True)

    if ok > 0:
        st.subheader("âœ… ä¸€è‡´ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨ä»¶ãƒ»ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
        matched_only = matched[matched["geom"]!="ä¸€è‡´ãªã—"].copy()
        lcol, rcol, _ = st.columns([1,1,4])
        with lcol:
            page_size = st.selectbox("ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º", [25, 50, 100, 200], index=1, key="pg_size")
        total_pages = max(1, math.ceil(len(matched_only)/page_size))
        with rcol:
            page = st.number_input("ãƒšãƒ¼ã‚¸", min_value=1, max_value=total_pages,
                                   value=min(st.session_state.get("pg_no", 1), total_pages), step=1, key="pg_no")
        page_df = paginate(matched_only, int(page), int(page_size))
        start_idx = (int(page)-1)*int(page_size)
        show_df = page_df.drop(columns=["geom"]).copy()
        show_df.insert(0, "No.", range(start_idx+1, start_idx+1+len(show_df)))
        st.dataframe(show_df, use_container_width=True, height=420)
        st.caption(f"å…¨ {len(matched_only):,} ä»¶ä¸­ã€{start_idx+1:,}â€“{start_idx+len(show_df):,} ä»¶ã‚’è¡¨ç¤º / {total_pages} ãƒšãƒ¼ã‚¸")

        st.download_button("ğŸ“¥ ã“ã®ãƒšãƒ¼ã‚¸ã‚’CSVã§ä¿å­˜",
            show_df.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"ä¸€è‡´ä¸€è¦§_p{page}_n{page_size}.csv", mime="text/csv"
        )

    # 5) ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‹åœ°å›³
    msg.text("å‡ºåŠ›ç”Ÿæˆâ€¦")
    st.download_button("ğŸ“¥ å…¨ä»¶Excel", to_excel_bytes(matched),
        file_name=f"{st.session_state.sheet_name}_ç…§åˆ_å…¨ä»¶.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

    um = matched[matched["geom"]=="ä¸€è‡´ãªã—"]
    if not um.empty:
        st.download_button("ğŸ“¥ æœªä¸€è‡´ã®ã¿", to_excel_bytes(um),
            file_name=f"{st.session_state.sheet_name}_æœªä¸€è‡´ã®ã¿.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

    mgdf = matched[matched["geom"]!="ä¸€è‡´ãªã—"]
    if not mgdf.empty:
        mgdf = gpd.GeoDataFrame(mgdf, geometry="geom", crs="EPSG:4326")
        m = folium.Map(zoom_start=14)
        folium.GeoJson(mgdf.__geo_interface__,
                       tooltip=folium.features.GeoJsonTooltip(fields=[excel_addr], aliases=["ä½æ‰€åœ°ç•ª"])
                      ).add_to(m)
        minx,miny,maxx,maxy = mgdf.total_bounds
        m.fit_bounds([[miny, minx],[maxy, maxx]])
        st.subheader("ğŸ—ºï¸ ä¸€è‡´ã—ãŸç­†ãƒãƒªã‚´ãƒ³"); folium_static(m, width=1100, height=680)
    else:
        st.warning("ä¸€è‡´ã™ã‚‹ç­†ãƒãƒªã‚´ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    # 100% å®Œäº†è¡¨ç¤º
    prog.progress(100); msg.text("å‡¦ç†å®Œäº† ğŸ‰")
